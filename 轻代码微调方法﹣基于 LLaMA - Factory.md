# 1ã€ LLaMA - Factory çš„å®‰è£…éƒ¨ç½²
```bash
    git clone https://github.com/hiyouga/LLaMA-Factory.git
```

# 2ã€ LLaMA - Factory å¿…å¤‡é¡¹å®‰è£…

```bash
pip3 install --upgrade pip 
pip3 install bitsandbytes >=0.39.0
cd LLaMA-Factory
#å®‰è£…æ ¸å¿ƒä¾èµ–ï¼ˆé»˜è®¤å¼€å¯FlashAttentionä¼˜åŒ–ï¼‰   
pip3 install -e ".[torch,metrics]"
```

# 3ã€ LLaMA - Factory çš„ä¸»è¦å­ç›®å½•è¯´æ˜

    config ï¼šå­˜æ”¾è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒè„šæœ¬
    data ï¼šå­˜æ”¾ç¤ºä¾‹çš„è®­ç»ƒæ•°æ®é›†æ ·æœ¬ï¼Œé‡‡ç”¨ json æ ¼å¼
    examples ï¼šå­˜æ”¾ç¤ºä¾‹çš„è®­ç»ƒè„šæœ¬ï¼ŒåŒ…æ‹¬ train_full ã€ train_lora ã€ train_qlora ã€ inferenceï¼ˆæ¨ç†ï¼‰ã€ merge_lora ï¼ˆæ¨¡å‹åˆå¹¶ï¼‰
    saves ï¼šå¾®è°ƒæ¨¡å‹ä¸´æ—¶å­˜æ”¾ç›®å½•
    models ï¼šæ¨¡å‹åˆå¹¶ä¸´æ—¶å­˜æ”¾ç›®å½•

# 4ã€è®­ç»ƒæ¨¡æ¿åˆ¶ä½œ

    åœ¨examplesç›®å½•ä¸‹æ‰¾åˆ°train_loraå­ç›®å½•ï¼Œå¤åˆ¶llama3_lora_sft.yamlæ–‡ä»¶çš„ä¸€ä¸ªå¤‡ä»½ï¼Œè¿›è¡Œä¿®æ”¹ï¼Œé‡æ–°å‘½åï¼Œæ¯”å¦‚deepseek_lora.yaml,å­˜æ”¾åˆ°configç›®å½•ä¸­

# 5ã€å‡†å¤‡æ•°æ®
æ¨¡æ¿è¦æ±‚ï¼ˆæ”¯æŒå•è½®/å¤šè½®å¯¹è¯ï¼‰ï¼š
```json
[
  {
    "conversation": [
      {"role": "user", "content": "å¦‚ä½•åšç•ªèŒ„ç‚’è›‹ï¼Ÿ"},
      {"role": "assistant", "content": "1. é¸¡è›‹æ‰“æ•£..."}
    ]
  },
  {
    "conversation": [
      {"role": "user", "content": "LLMå¾®è°ƒæœ‰å“ªäº›æ–¹æ³•?"},
      {"role": "assistant", "content": "å¸¸è§æ–¹æ³•åŒ…æ‹¬..."},
      {"role": "user", "content": "å“ªç§æ˜¾å­˜å ç”¨æœ€ä½ï¼Ÿ"},
      {"role": "assistant", "content": "LoRA+æ¢¯åº¦æ£€æŸ¥ç‚¹..."}
    ]
  }
]
```
æ•°æ®æ ¼å¼è½¬æ¢ï¼ˆå·²æœ‰åŸå§‹æ•°æ®æ—¶ï¼‰ï¼š
```bash
python scripts/preprocess_data.py \
  --input your_data.json \
  --output formatted_data.json \
  --template alpaca  # å¯é€‰æ¨¡æ¿: moss, chatglm3, qwen
```
é˜¶æ®µ3ï¼šå¯åŠ¨è®­ç»ƒï¼ˆ4ç§ç»å…¸åœºæ™¯é…ç½®ï¼‰
ğŸ“Œ åœºæ™¯1ï¼šä½æˆæœ¬å•å¡LoRAå¾®è°ƒ
```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
  --stage sft \
  --model_name_or_path meta-llama/Llama-2-7b-hf \
  --dataset formatted_data \
  --template default \
  --lora_target q_proj,v_proj  \  # å…³é”®ï¼šé€‰æ‹©LoRAæ³¨å…¥å±‚
  --per_device_train_batch_size 8 \ 
  --gradient_accumulation_steps 4 \
  --learning_rate 2e-5 \
  --max_grad_norm 0.5 \
  --val_size 0.1 \
  --max_samples 1000 \
  --fp16 \
  --output_dir outputs/llama2-lora
```
ğŸ“Œ åœºæ™¯2ï¼š8GBæ˜¾å¡QLoRAè®­ç»ƒ
```yaml
# ä¿®æ”¹train_args/qlora.yaml
quantization_bit: 4
lora_config:
  r: 64
  target_modules: ["q_proj","k_proj"]
train:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
```
```bash
python src/train_bash.py --config train_args/qlora.yaml
```
é˜¶æ®µ4ï¼šç»“æœéªŒè¯ï¼ˆ1åˆ†é’Ÿï¼‰
```python
from transformers import pipeline

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹ï¼ˆè‡ªåŠ¨èåˆLoRAæƒé‡ï¼‰
generator = pipeline("text-generation", 
                    model="outputs/llama2-lora/merged",
                    device=0)

question = "å¸®æˆ‘å†™å°è‹±æ–‡ä¼šè®®é‚€è¯·å‡½"
response = generator(question, 
                   max_new_tokens=300,
                   temperature=0.7,
                   do_sample=True)
print(response[0]['generated_text'])
```
âš¡ é«˜çº§æŠ€å·§
- 1.æ··åˆç²¾åº¦ä¼˜åŒ–ï¼šåœ¨3060ç­‰æ˜¾å¡å¯ç”¨--bf16å¯æé€Ÿ30%
- 2.æ˜¾å­˜èŠ‚çœï¼šæ·»åŠ --flash_attnå’Œ--gradient_checkpointingå¯é™ä½40%æ˜¾å­˜
- 3.å¹¶è¡Œè®­ç»ƒï¼šå¤šå¡è®­ç»ƒæ—¶å¢åŠ --ddp_find_unused_parameters False
- 4.ç›‘æ§æŒ‡æ ‡ï¼šTensorBoardå®æ—¶è®°å½•æŸå¤±æ›²çº¿
```bash
tensorboard --logdir outputs/llama2-lora/runs
```
ğŸ› ï¸ å¸¸è§é—®é¢˜æ’æŸ¥
é—®é¢˜ç°è±¡	è§£å†³æ–¹æ¡ˆ
OOMæ˜¾å­˜ä¸è¶³	é™ä½batch_sizeï¼Œå¢åŠ gradient_accumulation_steps
Lossä¸ä¸‹é™	æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œå°è¯•è°ƒå¤§learning_rateï¼ˆ1e-5â†’3e-5ï¼‰
ç”Ÿæˆç»“æœä¹±ç 	æ·»åŠ --model_name_or_pathä¸­çš„tokenizerè·¯å¾„
æ¨ç†é€Ÿåº¦æ…¢	å¯¼å‡ºONNXæ ¼å¼ï¼špython scripts/export_onnx.py

æœ€æ–°æ”¯æŒï¼šLLaMA-Factoryå·²æ”¯æŒ Llama3-8B/70B, Qwen1.5, DeepSeek-MoE ç­‰20+ä¸»æµæ¶æ„ï¼Œå¯åœ¨/modelsç›®å½•æŸ¥çœ‹å®Œæ•´æ”¯æŒåˆ—è¡¨ã€‚